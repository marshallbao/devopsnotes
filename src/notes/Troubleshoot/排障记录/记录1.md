# 事故记录 1

服务异常，exec -it pods 失败

环境：

gcloud环境

gke集群

表现：

raninbow服务异常接口调用失败，同事排查多次无果，最后重启rainbow-server pod，服务恢复正常；同事反应进入pod异常报 ssh: connect refuse;我这边也是一样的表现，进不去，进去被踢出来，页面卡住

结合最近gke集群总是在报警，但是一会就恢复正常了，加上前几天浏览器后端服务也是异常，多个方面排查无果后重启backend后服务恢复正常；

排查思路：

1、ssh进入pod 有问题，感觉是pod所在node的网络问题，想登录node查看网络资源如何（方向完全没问题）

2、进入节点后通过netstat -an,发现大量close_wait链接，有3w多个，通过netstat -anp看不到是哪个进程造成的

3、通过挨个进入pod、container查看，发现都没有大量close_wait，同时查看节点负载发现此节点负载不是很高，我们对照了高负载的节点发现也有类似的进程，但是数量很少；

3、后来才发现登录的用户不是root用户，于是通过sudo netstat -anp，发现是gke自己的进程，进程名字为gke-cdc30482304820342,大量类似的进程，造成了网络资源问题

4、同时在gke的管理页面发现此node的kubelet一直在重启（页面有提示），应该也是网络资源的问题

处理：

因为是生产环境我们没有大量kill此进程，于是通过标记此node为不可调度然后驱逐pod，将服务pod移动至其他节点，待pods启动后，检查服务无问题；然后忽然发现之前有问题的那台机器被gke删除掉了（应该是负载太低，<gke配置了节点自动扩缩容导致>）

原因：

就是gke的问题，但是具体啥问题，google也没有搜索到类似的问题，而且gke添加的节点是gcloud自己封装的，啥系统都不知道（机器只是node，只作为k8s的node,实际登录上去一点不好用），很多只读系统，无法写入东西；

反思：

排查问题思路：服务问题，首先排查资源，或者从最简单的方面开始排查；加强资源、服务接口、监控检查的监控、收集、报警工作；