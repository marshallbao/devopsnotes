kubernetes反应慢排查

kubernetes

版本：v1.19

部署方式：kubeadm

节点数量：3个

节点资源：4C8G

问题：

1、刚到公司，测试找过来说，应用（应用部署在kubernetes集群上）有问题，在处理应用问题的过程中发现在pod中读取编辑文件的很慢，因为kubernetes使用的存储是ceph，ceph集群的资源有限，内存已经已经跑满了，我们开始以为是ceph导致的；

2、但是慢慢发现整个kubernetes集群也变得很慢，于是将集群中暂时不用的服务开始停掉，但是没有效果，api 和etcd开始出现超时的现象，整个集群操作有了问题，但是pod还在正常运行，于是我重启了kubernetes集群的节点，重启之后发现api 和etcd还是异常，尤其是api自己启动然后自己又挂掉；通过查看api的日志并没有发现特别的异常，于是开始查看主机状态发现主机负载很高,已经达到15左右，内存正常，通过top等工具发现cpu实际使用率不高，io wait很高，一直在左右90%，我们还是怀疑ceph的问题，然后同一个服务（都使用cephfs）的3个实例都启动在一个node 上,我开始使用pods亲和性，将pods分别启动在不同主机，也没有效果；

3、然后测试着急测试服务，开始去另一个环境给他部署服务，耽误了挺长时间，下午的时候，运维同事说在同一台物理机的其他虚拟机，部署了大量服务，于是我们判定是虚拟机之间的隔离不彻底，导致的虚拟机的负载问题

4、接着我重启ceph服务，但是有几个实例没有起来，ceph也开始有问题；然后同事开始慢慢将他部署在其他虚拟机的服务迁移出去，主节点负载最终稳定在5左右，还是很高，因为现在集群中基本没有服务了，kubectl使用正常但是没有很流畅

5、我开始使用iostat,pidstat,mpstat,工具开始排查io wait 问题，主机点的io主要是etcd，mongo(mogog pod启动在主节点连接ceph）;但是具体的进程找不到；然后去物理机上查找io,只发现几个进程占用io,但是无法确定；io排查到此结束，目前服务可以运行，但是负载不正常；

6、第二天同事让我新建nfs作为kubernetes集群的存储后端，我做好nfs后，尝试重启ceph实例的主机，ceph启动正常，kubernetes主节点的负载也瞬间下去了，稳定在1左右；到目前为止整个故障处理完成了；

故障回顾及整理

根本原因：资源不够

  1、ceph集群资源不够，内存一直跑的很满，无法排除ceph的问题

  2、同事在同一台物理机上启动大量服务，导致整个节点负载很重

故障排查的错误点：

  1、文件读取慢的时候，可能是ceph的问题，但是pods所在节点的io，可能也有问题；

  2、整个kubernetes集群反应慢的时候，不应该去重启，而是要查看主机负载，主要是看内存，cpu的实际使用率和io wait

  3、回顾看来其实很简单，主要是ceph的不正常，导致无法排除这个变量，影响判断；

故障排查流程收获：

  1、服务慢第一反应是主机资源，cpu，内存，io wait;

  2、多个故障点的时候不好判断，因为没有唯一变量，尤其是资源问题，通过日志查不出来；

  3、主机负载的计算是实际cpu的计算量+io wait+不可中断状态的进程（实际排查工具和思路写在性能篇）；

  4、io wait的问题不好判断是某一个进程，可以确定一部分进程，通过停止或者重启这部分进程来解决；

其他：

  1、故障排查流程不明确；耽误了很多时间

  2、虚拟机还是不靠谱，隔离没有那么彻底

  3、同事之间的沟通不流畅，刚开始我是不知道，新部署这么多服务，同事也没有意识到虚拟机之间会影响；

  4、排查故障和业务恢复的优先级的确定；测试一直在催我，影响我处理故障

  5、没有处理过资源或者性能方面的经验