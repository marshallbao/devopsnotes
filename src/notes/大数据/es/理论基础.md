# Elasticsearch

[ElasticSearch](https://so.csdn.net/so/search?q=ElasticSearch&spm=1001.2101.3001.7020) 简称 es，开源的分布式的全文搜索引擎，可以近乎实时的存储检索数据，es使用java开发，并且使

用 Lucene 作为核心实现搜索功能。

分布式的[搜索引擎](https://so.csdn.net/so/search?q=搜索引擎&spm=1001.2101.3001.7020)，全文检索，数据分析引擎（分组聚合），对海量数据实时处理

### 相关概念

##### 索引（index）

索引是ES的一个逻辑存储，对应关系型数据库中的库，ES可以把索引数据存放到服务器中，也可以sharding(分片)后存储到多台服务器上。每个索引有一个或多个分片，每个分片可以有多个副本。

##### 类型（type）

ES中，一个索引可以存储多个用于不同用途的对象，可以通过类型来区分索引中的不同对象，对应关系型数据库中表的概念。但是在ES6.0开始，类型的概念被废弃，ES7中将它完全删除。删除type的原因：

我们一直认为ES中的“index”类似于关系型数据库的“database”，而“type”相当于一个数据表。ES的开发者们认为这是一个糟糕的认识。例如：关系型数据库中两个数据表示是独立的，即使他们里面有相同名称的列也不影响使用，但ES中不是这样的。

我们都知道elasticsearch是基于Lucene开发的搜索引擎，而ES中不同type下名称相同的filed最终在Lucene中的处理方式是一样的。举个例子，两个不同type下的两个user_name，在ES同一个索引下其实被认为是同一个filed，你必须在两个不同的type中定义相同的filed映射。否则，不同type中的相同字段名称就会在处理中出现冲突的情况，导致Lucene处理效率下降。

去掉type能够使数据存储在独立的index中，这样即使有相同的字段名称也不会出现冲突，就像ElasticSearch出现的第一句话一样“你知道的，为了搜索····”，去掉type就是为了提高ES处理数据的效率。

除此之外，在同一个索引的不同type下存储字段数不一样的实体会导致存储中出现稀疏数据，影响Lucene压缩文档的能力，导致ES查询效率的降低

###### 文档（document）

存储在ES中的主要实体叫文档，可以理解为关系型数据库中表的一行数据记录。每个文档由多个字段（field）组成。区别于关系型数据库的是，ES是一个非结构化的数据库，每个文档可以有不同的字段，并且有一个唯一标识。

###### 映射（mapping）

mapping是对索引库中的索引字段及其数据类型进行定义，类似于关系型数据库中的表结构。ES默认动态创建索引和索引类型的mapping，这就像是关系型数据中的，无需定义表机构，更不用指定字段的数据类型。当然也可以手动指定mapping类型。


###### Cluster

集群，一个ES集群是由多个节点(Node)组成的，每个集群都有一个cluster name 作为标识
cluster.name: 【elastic search cluster name】
在同一网段下的Elastic search实例会通过cluster name 决定加入哪个集群下。

###### node

节点，一个ES实例就是一个node，一个机器可以有多个实例，所以并不是说一台机器就是一个node，大多数情况下，每个node运行在一个独立的环境或者虚拟机上。

###### index

索引，即一系列documents的集合

###### shard

1.分片，ES是分布式搜索引擎，每个索引有一个或多个分片，索引的数据被分配到各个分片上，相当于一桶水   用了N个杯子装
2.分片有助于横向扩展，N个分片会被尽可能平均地（rebalance）分配在不同的节点上（例如你有2个节点，4个主分片(不考虑备份)，那么每个节点会分到2个分片，后来你增加了2个节点，那么你这4个节点上都会有1个分片，这个过程叫relocation，ES感知后自动完成)
3.分片是独立的，对于一个Search Request的行为，每个分片都会执行这个Request.另外
4.每个分片都是一个Lucene Index，所以一个分片只能存放 Integer.MAX_VALUE - 128 = 2,147,483,519个docs。

###### replica

1. 复制，可以理解为备份分片，相应地有 primary shard（主分片）

2. 主分片和备分片不会出现在同一个节点上（防止单点故障），默认情况下一个索引创建 5 个分片一个备份（即5 primary+5 replica= 10个分片）

3. 如果你只有一个节点，那么5个replica都无法分配（unassigned），此时cluster status会变成Yellow。

replica 的作用主要包括: 

1. 容灾：primary 分片丢失，replica 分片就会被顶上去成为新的主分片，同时根据这个新的主分片创建新的replica，集群数据安然无恙 

2. 提高查询性能：replica 和 primary 分片的数据是相同的，所以对于一个 query 既可以查主分片也可以查备分片，在合适的范围内多个 replica 性能会更优（但要考虑资源占用也会提升[cpu/disk/heap]），另外index request只能发生在主分片上，replica不能执行index request。 对于一个索引，除非重建索引否则不能调整分片的数目（主分片数, number_of_shards），但可以随时调整replica数(number_of_replicas)。



## **存储空间**

#### 分片策略

##### 1.1分布式架构的透明隐藏属性

Elasticsearch是一个分布式架构，隐藏了复杂的处理机制

###### 分片机制

一个索引默认有 5 个主分片，每个主分片默认有 1 个副本分片，即创建一个索引默认会有 10 个分片，我们不用关心数据是按照什么机制分片的，最后放到哪个分片中

###### 分片的副本

为了提升访问压力过大是单机无法处理所有请求的问题，Elasticsearch集群引入了副本策略replica。副本策略对index中的每个分片创建冗余的副本，处理查询时可以把这些副本当做主分片来对待（primary shard），此外副本策略提供了高可用和数据安全的保障，当分片所在的机器宕机，Elasticsearch可以使用其副本进行恢复，从而避免数据丢失。

###### 集群发现机制(cluster discovery)

比如当前我们启动了一个es进程，当启动了第二个es进程是，这个进程作为一个node自动就发现了集群，并加入进去。

Shard负载均衡：比如现在有10个shard，集群中有3个节点，es会均衡的进行分配，以保证每个节点均衡的负载请求，

请求路由：当索引一个文档的时候，文档会被存储到一个主分片中。 Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？

首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。实际上，这个过程是根据下面这个公式决定的：

`shard = hash(routing) % number_of_primary_shards`

routing 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。 routing 通过 hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到 余数 。这个分布在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。

#### 扩容机制



#### 容灾原理

1. ES 中的 index，首先会进行分片，每个分片数据通常都会有本身的副本数据，ES分配分片的策略会保证同一个分片数据和本身的副本不会分配到同一个节点上
2. 当集群中的某一节点宕机后，ES 的 master 在 ping 该节点时经过必定的策略会发现该节点不存活；此时，ES开启恢复过程，恢复的策略以下：
   2.1.  恢复的目标是保证集群中分片的副本数不变
   2.2.  若是宕机的节点上承载某分片的主分片，那么此时（恢复过程）会将该分片分配在其余节点上的某一副本提高为主分片（记住：同一分片和其副本老是不在同一节点上，那么此事是保证有对应的副本可供提高的）
   2.3.  根据 1 保证副本数不变，若是宕机的节点承载某分片的副本，那么 ES 会在其余非宕机节点上用主分片复制一个副本
   2.4.  整个过程不影响集群的读写功能；可是因为多了复制分片和迁移分片的过程，集群的读写性能受影响 



### 参考

https://mp.weixin.qq.com/s?__biz=MzIwMDU3MDkzNg==&mid=2651321248&idx=3&sn=b11a2d871e0ca01282f27107d7e31e88&chksm=8d0872f7ba7ffbe1b39fc9bbbf730c08b1e2eb5a0d72b7fa40376e7389a02035a53725aa7e4b&scene=27