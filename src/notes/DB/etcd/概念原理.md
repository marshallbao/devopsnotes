# etcd 

etcd 的核心是 **分布式一致性 + 高可用存储**，主要包括：

1. **Raft 协议保证一致性**
   - etcd 集群内有多个节点（一般是奇数 3/5 个）。
   - 通过 Raft 算法选出一个 Leader，所有写操作都必须经过 Leader，再复制到 Follower。
   - 这样保证了所有节点在任何时刻数据都是一致的（强一致性）。
2. **写入流程**
   - 客户端写数据 → Leader 节点写入 WAL（Write-Ahead Log）
   - Leader 将日志通过 Raft 复制到其他 Follower
   - 半数以上节点确认后，写操作才算成功（保证了容错性）
3. **存储引擎（bbolt）**
   - 数据存储在一个单一文件（类似 SQLite），用 B+Tree 索引管理 KV。
   - 由于使用 **内存映射文件（mmap）**，所以读性能非常高。
   - 每次修改会生成新的 B+Tree 节点，旧的会在 compaction 后被清理。
4. **快照与压缩（Compaction）**
   - 随着 Raft 日志增长，历史版本会越来越多。
   - etcd 会定期执行 **快照 + compaction**，只保留最新版本和必要的日志，避免存储无限增长。

### 关于bbolt

bbolt 就是 一个 纯 Go 实现的，基于 B+Tree 的嵌入式 KV 引擎，数据放在 mmap 文件里，读快，写安全，适合小容量、强一致的场景，但不适合大规模存储。



### Raft 协议

Raft 算法是一种用于分布式系统中复制日志一致性管理的算法。它通过选举领导者来协调日志复制，确保所有节

点数据一致。算法包括心跳机制、选举过程、日志复制和一致性保证。当领导者失效时，节点会重新选举，保证高

可用性。Raft易于理解和实现，提供强一致性，常用于分布式数据库和协调服务

心跳机制 &选举机制

leader 会定期向 follower 发送心跳；如果 follower 一段时间内收不到 leader 发送的心跳；那么 follower 会转成 candidate 来进行选举选出 leader(只有获得majority投票的节点才会成为leader)；

日志复制

客户端向 leader 发送消息，leader 收到后会同步给所有 follower 等大多数 follower 回复后，master 才 commit 次此消息并给客户端 ACK;

 一致性保证

1. 领导者通过日志复制 RPC 的一致性检查，找到跟随者与自己相同日志项的最大索引值。即在该索引值之前

   的日志，领导者和跟随者是一致的，之后的日志，就不一致了

2. 领导者强制将跟随者该索引值之后的所有日志项删除，并将领导者该索引值之后的所有日志项同步至跟随者，以实现日志的一致

问题

当 leader 收到大多数 follower 的确认后，给客户端反馈了之后就挂掉了；但是还没给 follower 发送心跳， Follower 处于未提交状态，这时候要怎么处理？？？？？

### 参考

https://www.cnblogs.com/mindwind/p/5231986.html

https://developer.baidu.com/article/details/2984937

https://developer.aliyun.com/article/1519571



https://blog.csdn.net/ctypyb2002/article/details/132380678