### minikube搭建 


1、安装kubectl
自行百度，建议github上找二进制文件，下载后上传至服务器/usr/local/bin/
2、安装minikube
curl -Lo minikube https://github.com/kubernetes/minikube/releases/download/v1.9.2/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/

3、启动minkube

minikube start --driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --force --cpus 1
加网络代理代理
minikube start --driver=none --extra-config=kubeadm.ignore-preflight-errors=NumCPU --force --cpus 1 --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers

4、停止与删除

minikube stop
minikube delete  删除本地集群
minikube delete all  删除所有本地群集和配置文件



### kubeadm搭建




1、安装docker
下载docker yum源 
wget http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  && mv docker-ce.repo /etc/yum.repos.d/
安装docker
yum install docker-ce-18.09.8-3.el7.x86_64

systemctl start docker && systemctl enable docker

编辑 /etc/docker/daemon.json

2、安装kubernetes

下载kubernetes源

cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

按照顺序安装：（）
yum install kubelet-1.15.0-0.x86_64(依赖kubernetes-cni-0.8.6-0.x86_64)
yum install kubectl-1.15.0-0.x86_64
yum install kubeadm-1.15.0-0.x86_64
systemctl enable kubelet && systemctl start kubelet

3、拉取镜像
master:
kube-apiserver:v1.15.0
kube-controller-manager:v1.15.0
kube-scheduler:v1.15.0
kube-proxy:v1.15.0
pause:3.1
etcd:3.3.10
coredns:1.3.1
quay.io/coreos/flannel:v0.12.0-amd64

node:
kube-proxy:v1.15.0
pause:3.1
quay.io/coreos/flannel:v0.12.0-amd64  #根据启动flaanel的yaml文件

vim pullImages.sh
\#!/bin/bash
url=registry.cn-hangzhou.aliyuncs.com/google_containers #阿里云镜像仓库地址，可以按需修改
version=v1.16.4 #安装的kubernetes的版本（可以按需修改）
images=(`kubeadm config images list --kubernetes-version=$version|awk -F '/' '{print $2}'`)
for imagename in ${images[@]} ; do
  docker pull $url/$imagename
  docker tag $url/$imagename k8s.gcr.io/$imagename
  docker rmi -f $url/$imagename
done

sh pullImages.sh

4、初始化
初始化：
kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --apiserver-cert-extra-sans=172.16.44.42,172.17.0.1,101.132.180.28 --apiserver-advertise-address=101.132.180.28 --ignore-preflight-errors=NumCPU --image-repository registry.aliyuncs.com/google_containers

如果初始化有问题可以，执行kubeadm reset 重置节点（master和node都可以执行）

记录下来初始化成功的输出
kubeadm join 172.16.44.42:6443 --token c1er4s.0epvootzm019unno \
    --discovery-token-ca-cert-hash sha256:ee39785d1423aa77ebb1959f6d1bd7e0fa60226ef0dd2c629f0cf8a1d5892351

如果忘记记录，或者token过期
kubeadm token create #生产新的token

kubelet启动失败，可通过journalctl -xe，查看日志，是不是因为docker的Cgroup driver和kubelet  Cgroup driver 不一致



5、配置网络
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

6、node节点
安装以上步骤执行安装 docker,配置 kubernetes 源，按照顺序安装 kubelet,kubectl,kubeadm,systemctl enable kubelet && systemctl start kubelet

加入集群
kubeadm join 172.16.44.42:6443 --token c1er4s.0epvootzm019unno \
    --discovery-token-ca-cert-hash sha256:ee39785d1423aa77ebb1959f6d1bd7e0fa60226ef0dd2c629f0cf8a1d5892351 -v 3  #查看加入的详细日志防止出现问题

kubeadm token create --print-join-command #重新生成加入集群命令


### 二进制搭建 


kubernetes二进制安装
【info】
master 10.255.0.127 116.85.35.9
node1  10.255.0.66  116.85.29.214
node2  10.255.0.92  116.85.17.88
45rtfgVB@

【初始化服务器】
	1、关闭防火墙

​	2、关闭selinux
​	临时：setenforce 0
​	永久（需要重启主机）：cat /etc/selinux/config
​						  SELINUX=disabled
​	3、配置主机名及hosts
​		hostnamectl set-hostname master
​		hostnamectl set-hostname node1
​		hostnamectl set-hostname node2
​		vim /etc/hsots
​			10.255.0.127   master
​			10.255.0.66    node1 
​			10.255.0.92    node2

​	4、配置时间同步
​	5、关闭swap
​	6、配置相关内核参数（这个暂时没有接触到）
【目录】
[root@master cfssl]# mkdir -p /data/{kubernetes,etcd,kubelet,kube-proxy,flannel}
[root@master cfssl]# mkdir -p /opt/{kubernetes,cfssl,etcd,cni,flannel}
[root@master cfssl]# mkdir -p /opt/kubernetes/{cfg,bin,ssl,log}
[root@master cfssl]# mkdir -p /opt/etcd/{cfg,ssl}

【docker】
[root@node1 cfg]# cd /etc/yum.repos.d/
[root@node1 yum.repos.d]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
[root@node1 yum.repos.d]# yum install docker-ce
[root@node1 yum.repos.d]# systemctl enable docker
[root@node1 yum.repos.d]# systemctl start docker

【CA证书】
安装cfssl

[root@master cfssl]# cd /opt/cfssl/
[root@master cfssl]# wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
[root@master cfssl]# wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
[root@master cfssl]# wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
[root@master cfssl]# chmod +x cfssl*
[root@master cfssl]# mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo
[root@master cfssl]# mv cfssljson_linux-amd64  /usr/local/bin/cfssljson
[root@master cfssl]# mv cfssl_linux-amd64  /usr/local/bin/cfssl

copy工具：
scp  /usr/local/bin/cfssl* root@116.85.29.214:/usr/local/bin/

CA
1、创建用来生成 CA 文件的 JSON 配置文件 ca-config.json（具体内容还待研究）
[root@master cfssl]# cat > ca-config.json <<EOF
\> {
\>   "signing": {
\>     "default": {
\>       "expiry": "87600h"
\>     },
\>     "profiles": {
\>       "kubernetes": {
\>          "expiry": "87600h",
\>          "usages": [
\>             "signing",
\>             "key encipherment",
\>             "server auth",
\>             "client auth"
\>         ]
\>       }
\>     }
\>   }
\> }
\> EOF
2、创建用来生成 CA 证书签名请求（CSR）的 JSON 配置文件
[root@master cfssl]# cat > ca-csr.json <<EOF
\> {
\>     "CN": "kubernetes",
\>     "key": {
\>         "algo": "rsa",
\>         "size": 2048
\>     },
\>     "names": [
\>         {
\>             "C": "CN",
\>             "L": "Beijing",
\>             "ST": "Beijing",
\>             "O": "k8s",
\>             "OU": "System"
\>         }
\>     ]
\> }
\> EOF

以上就是关于ca的证书相关文件源文件有ca-config.json,ca-csr.json （ca-config.json每次生成证书都会需要）
3、生成CA证书（ca.pem）和密钥（ca-key.pem）
[root@master cfssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
会生成以下文件（CA证书，CA私钥）
-rw-r--r-- 1 root root 1359 Sep 14 14:59 ca.pem
-rw------- 1 root root 1679 Sep 14 14:59 ca-key.pem
-rw-r--r-- 1 root root 1001 Sep 14 14:59 ca.csr
4、copy证书
[root@master cfssl]# cp ca.pem ca-key.pem ca.csr /opt/kubernetes/ssl/
[root@master cfssl]# scp ca.pem ca-key.pem ca.csr root@116.85.29.214:/opt/kubernetes/ssl/
[root@master cfssl]# scp ca.pem ca-key.pem ca.csr root@116.85.17.88:/opt/kubernetes/ssl/

下面启动服务都分2个部分.证书与服务；
【etcd】
创建etcd集群	
	证书
	创建etcd请求证书文件
[root@master cfssl]# cat > etcd-csr.json <<EOF
\> {
\>   "CN": "etcd",
\>   "hosts": [
\>     "127.0.0.1",
\>     "10.255.0.127",
\>     "10.255.0.66",
\>     "10.255.0.92"
\>   ],
\>   "key": {
\>     "algo": "rsa",
\>     "size": 2048
\>   },
\>   "names": [
\>     {
\>       "C": "CN",
\>       "ST": "BeiJing",
\>       "L": "BeiJing",
\>       "O": "k8s",
\>       "OU": "System"
\>     }
\>   ]
\> }
\> EOF
	生成证书及私钥
[root@master ssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem   -ca-key=/opt/kubernetes/ssl/ca-key.pem   -config=/opt/kubernetes/ssl/ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
	产生文件
	-rw-r--r-- 1 root root 1436 Sep 14 15:45 etcd.pem
	-rw------- 1 root root 1675 Sep 14 15:45 etcd-key.pem
	-rw-r--r-- 1 root root 1062 Sep 14 15:45 etcd.csr

​	分发	
[root@master ssl]# scp etcd-key.pem etcd.pem  root@116.85.29.214:/opt/kubernetes/ssl/	
[root@master ssl]# scp etcd-key.pem etcd.pem  root@116.85.17.88:/opt/kubernetes/ssl/	
​	服务
​	github下载etcd二进制文件上传至服务器/opt/etcd/etcd-v3.3.25-linux-amd64.tar.gz，解压至当前文件夹cd etcd-v3.3.25-linux-amd64.tar.gz(3个节点都这样操作)
[root@master etcd-v3.3.25-linux-amd64]# cp etcd etcdctl  /usr/local/bin/	
​	编辑配置文件（3个节点）
​	/opt/etcd/cfg/etcd.conf
[root@master cfg]# cat etcd.conf 
\#[member]
ETCD_NAME="etcd-node1"
ETCD_DATA_DIR="/data/etcd"
\#ETCD_SNAPSHOT_COUNTER="10000"
\#ETCD_HEARTBEAT_INTERVAL="100"
\#ETCD_ELECTION_TIMEOUT="1000"
ETCD_LISTEN_PEER_URLS="[https://10.255.0.127:2380"](https://10.255.0.127:2380)               #节点间的通信
ETCD_LISTEN_CLIENT_URLS="[https://10.255.0.127:2379,https://127.0.0.1:2379"](https://10.255.0.127:2379,https://127.0.0.1:2379)  #客户端访问的url
\#ETCD_MAX_SNAPSHOTS="5"
\#ETCD_MAX_WALS="5"
\#ETCD_CORS=""
\#[cluster]
ETCD_INITIAL_ADVERTISE_PEER_URLS="[https://10.255.0.127:2380"](https://10.255.0.127:2380)
\# if you use different ETCD_NAME (e.g. test),
\# set ETCD_INITIAL_CLUSTER value for this name, i.e. "test=[http://..."](http://...)
ETCD_INITIAL_CLUSTER="etcd-node1=[https://10.255.0.127:2380,etcd-node2=https://10.255.0.66:2380,etcd-node3=https://10.255.0.92:2380"](https://10.255.0.127:2380,etcd-node2=https://10.255.0.66:2380,etcd-node3=https://10.255.0.92:2380)
ETCD_INITIAL_CLUSTER_STATE="new"
ETCD_INITIAL_CLUSTER_TOKEN="k8s-etcd-cluster"
ETCD_ADVERTISE_CLIENT_URLS="[https://10.255.0.127:2379"](https://10.255.0.127:2379)
\#[security]
CLIENT_CERT_AUTH="true"
ETCD_CA_FILE="/opt/kubernetes/ssl/ca.pem"
ETCD_CERT_FILE="/opt/kubernetes/ssl/etcd.pem"
ETCD_KEY_FILE="/opt/kubernetes/ssl/etcd-key.pem"
PEER_CLIENT_CERT_AUTH="true"
ETCD_PEER_CA_FILE="/opt/kubernetes/ssl/ca.pem"
ETCD_PEER_CERT_FILE="/opt/kubernetes/ssl/etcd.pem"
ETCD_PEER_KEY_FILE="/opt/kubernetes/ssl/etcd-key.pem"

​	编辑system service文件
[root@master cfg]# cd /usr/lib/systemd/system/
[root@master system]# cat etcd.service 
[Unit]
Description=Etcd Server
After=network.target

[Service]
Type=simple
WorkingDirectory=/data/etcd
EnvironmentFile=-/opt/etcd/cfg/etcd.conf
\# set GOMAXPROCS to number of processors
ExecStart=/bin/bash -c "GOMAXPROCS=$(nproc) /usr/local/bin/etcd"
Type=notify

[Install]
WantedBy=multi-user.target
	启动etcd集群
[root@node2 cfg]# systemctl daemon-reload
[root@node2 cfg]# systemctl enable etcd
[root@node2 cfg]# systemctl restart etcd
[root@node2 cfg]# systemctl status etcd
	检查etcd集群
[root@master lib]# etcdctl --endpoints=https://10.255.0.127:2379   --ca-file=/opt/kubernetes/ssl/ca.pem   --cert-file=/opt/kubernetes/ssl/etcd.pem   --key-file=/opt/kubernetes/ssl/etcd-key.pem cluster-health
member 121a2f66f2ff03f8 is healthy: got healthy result from https://10.255.0.92:2379
member 232fa3adff8a1cad is healthy: got healthy result from https://10.255.0.66:2379
member 778dbb77a455c563 is healthy: got healthy result from https://10.255.0.127:2379
cluster is healthy

etcd搭建完毕

<服务>
github下载kubernetes二进制文件，在release页面选择版本，进行下载kubernetes-server，kubernetes-node，kubernestes-client
https://storage.googleapis.com/kubernetes-release/release/v1.18.8/kubernetes-node-linux-amd64.tar.gz
https://storage.googleapis.com/kubernetes-release/release/v1.18.8/kubernetes-server-linux-amd64.tar.gz
https://storage.googleapis.com/kubernetes-release/release/v1.18.8/kubernetes-client-linux-amd64.tar.gz

软件包
[root@master bin]# mv kube-apiserver /usr/local/bin/
[root@master bin]# mv kube-scheduler /usr/local/bin/
[root@master bin]# mv kube-controller-manager /usr/local/bin/
[root@master bin]# mv kubectl /usr/local/bin/

【api-server】
创建apiserver请求证书文件
[root@master cfssl]# cat > kubernetes-csr.json <<EOF
\> {
\>   "CN": "kubernetes",
\>   "hosts": [
\>     "127.0.0.1",
\>     "10.255.0.127",
\>     "10.1.0.1",
\>     "kubernetes",
\>     "kubernetes.default",
\>     "kubernetes.default.svc",
\>     "kubernetes.default.svc.cluster",
\>     "kubernetes.default.svc.cluster.local"
\>   ],
\>   "key": {
\>     "algo": "rsa",
\>     "size": 2048
\>   },
\>   "names": [
\>     {
\>       "C": "CN",
\>       "ST": "BeiJing",
\>       "L": "BeiJing",
\>       "O": "k8s",
\>       "OU": "System"
\>     }
\>   ]
\> }
\> EOF
生成证书及私钥
[root@master cfssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem    -ca-key=/opt/kubernetes/ssl/ca-key.pem    -config=/opt/kubernetes/ssl/ca-config.json    -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes
分发
[root@master cfssl]# cp kubernetes.pem kubernetes-key.pem /opt/kubernetes/ssl/
[root@master cfssl]# scp kubernetes.pem kubernetes-key.pem root@10.255.0.66:/opt/kubernetes/ssl/
[root@master cfssl]# scp kubernetes.pem kubernetes-key.pem root@10.255.0.92:/opt/kubernetes/ssl/

创建 kube-apiserver 客户端使用的token 文件（就是kubelet初次认证的文件）
[root@master cfssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' '
0bc572d207243737a49868f0bc939a70
[root@master cfssl]# cat /opt/kubernetes/ssl/bootstrap-token.csv
0bc572d207243737a49868f0bc939a70,kubelet-bootstrap,10001,"system:kubelet-bootstrap"

创建基础用户名/密码认证配置
[root@master cfssl]# cat /opt/kubernetes/ssl/basic-auth.csv
admin,admin,1
readonly,readonly,2

​	//注意：token文件和basic-auth.csv 还不太清楚是做什么的（下面kublelet中也使用了这个token值，大体猜到是让Controller Manager，Scheduler第一次做认证使用的，但是因为在一个节点上所以没有用到，具体还不太清楚）

编辑apiserver system service文件（内容等下再详细了解）
[root@master system]# cat kube-apiserver.service 
[unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --bind-address=10.255.0.127 \
  --insecure-bind-address=127.0.0.1 \
  --authorization-mode=Node,RBAC \
  --runtime-config=rbac.authorization.k8s.io/v1 \
  --kubelet-https=true \
  --anonymous-auth=false \
  --basic-auth-file=/opt/kubernetes/ssl/basic-auth.csv \
  --enable-bootstrap-token-auth \
  --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv \
  --service-cluster-ip-range=10.1.0.0/16 \
  --service-node-port-range=20000-40000 \
  --tls-cert-file=/opt/kubernetes/ssl/kubernetes.pem \
  --tls-private-key-file=/opt/kubernetes/ssl/kubernetes-key.pem \
  --client-ca-file=/opt/kubernetes/ssl/ca.pem \
  --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \
  --etcd-cafile=/opt/kubernetes/ssl/ca.pem \
  --etcd-certfile=/opt/kubernetes/ssl/kubernetes.pem \
  --etcd-keyfile=/opt/kubernetes/ssl/kubernetes-key.pem \
  --etcd-servers=https://10.255.0.127:2379,https://10.255.0.66:2379,https://10.255.0.92:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/opt/kubernetes/log/api-audit.log \
  --event-ttl=1h \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
	启动服务
[root@master system]# systemctl daemon-reload
[root@master system]# systemctl enable kube-apiserver
[root@master system]# systemctl start kube-apiserver
[root@master system]# systemctl status kube-apiserver

【kube-controller-manager】
编辑kube-controller-manager system service文件

[root@master system]# cat kube-controller-manager.service 
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/opt/kubernetes/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.1.0.0/16 \
  --cluster-cidr=10.2.0.0/16 \
  --cluster-name=kubernetes \
  --cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \
  --cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem \
  --service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \
  --root-ca-file=/opt/kubernetes/ssl/ca.pem \
  --leader-elect=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
启动服务
[root@master system]# systemctl daemon-reload
[root@master system]# systemctl enable kube-controller-manager
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-controller-manager.service to /usr/lib/systemd/system/kube-controller-manager.service.
[root@master system]# systemctl start kube-controller-manager
[root@master system]# systemctl status kube-controller-manager

【kube-scheduler】
	编辑 kube-scheduler system service 文件

[root@master system]# cat /usr/lib/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --leader-elect=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
	启动服务
[root@master system]# systemctl daemon-reload
[root@master system]# systemctl enable kube-scheduler
Created symlink from /etc/systemd/system/multi-user.target.wants/kube-scheduler.service to /usr/lib/systemd/system/kube-scheduler.service.
[root@master system]# systemctl start kube-scheduler
[root@master system]# systemctl status kube-scheduler


kubectl命令行工具
【kubectl】
	创建 admin 证书签名请求
[root@master cfssl]# cat > admin-csr.json <<EOF
\> {
\>   "CN": "admin",
\>   "hosts": [],
\>   "key": {
\>     "algo": "rsa",
\>     "size": 2048
\>   },
\>   "names": [
\>     {
\>       "C": "CN",
\>       "ST": "BeiJing",
\>       "L": "BeiJing",
\>       "O": "system:masters",
\>       "OU": "System"
\>     }
\>   ]
\> }
\> EOF

\---
	注意：admin-csr.json内容要 CN就是用户名，O就是Group,也就是说在这里user:admin 和group:system:master关联起来了；而system:masters这个组默认绑定了cluster-admin这个clusterrole，cluster-admin clusterrole 是kubernetes集群中权限最大的；
[root@master .kube]# kubectl get clusterrole -n kube-system
NAME                                                                   CREATED AT
admin                                                                  2020-09-14T11:18:28Z
cluster-admin                                                          2020-09-14T11:18:28Z

[root@master .kube]# kubectl get clusterrolebinding -n kube-system
NAME                                                   ROLE                                                               AGE
cluster-admin                                          ClusterRole/cluster-admin                                          2d3h

\---

​	生成 admin 证书和私钥
[root@master cfssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem    -ca-key=/opt/kubernetes/ssl/ca-key.pem    -config=/opt/kubernetes/ssl/ca-config.json    -profile=kubernetes admin-csr.json | cfssljson -bare admin
[root@master cfssl]# mv admin.pem admin-key.pem /opt/kubernetes/ssl/

​	设置集群参数
[root@master cfssl]# kubectl config set-cluster kubernetes \
\>    --certificate-authority=/opt/kubernetes/ssl/ca.pem \
\>    --embed-certs=true \
\>    --server=https://10.255.0.127:6443

​	设置客户端认证参数
[root@master cfssl]# kubectl config set-credentials admin \
\>    --client-certificate=/opt/kubernetes/ssl/admin.pem \
\>    --embed-certs=true \
\>    --client-key=/opt/kubernetes/ssl/admin-key.pem

​	设置上下文参数
[root@master cfssl]# kubectl config set-context kubernetes \
\>    --cluster=kubernetes \
\>    --user=admin

​	设置默认上下文
[root@master cfssl]# kubectl config use-context kubernetes

​	测试
[root@master cfssl]# kubectl get cs
NAME                 STATUS    MESSAGE             ERROR
controller-manager   Healthy   ok                  
scheduler            Healthy   ok                  
etcd-0               Healthy   {"health":"true"}   
etcd-1               Healthy   {"health":"true"}   
etcd-2               Healthy   {"health":"true"}   

到此位置master服务部署完毕；
以上其中kubectl的config命令不太熟悉，以后要熟悉一下；
\-------------------------------------------------------------------------------------------
node节点部署
准备包
[root@node1 bin]# mv kubectl kubelet kube-proxy /usr/local/bin/
【kubelet】	
主节点操作
	创建角色绑定
[root@master cfssl]# kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
	创建 kubelet bootstrapping kubeconfig 文件 设置集群参数
[root@master cfssl]# kubectl config set-cluster kubernetes \
\>    --certificate-authority=/opt/kubernetes/ssl/ca.pem \
\>    --embed-certs=true \
\>    --server=https://10.255.0.127:6443 \
\>    --kubeconfig=bootstrap.kubeconfig

​	设置客户端认证参数
[root@master cfssl]# kubectl config set-credentials kubelet-bootstrap \
\>    --token=0bc572d207243737a49868f0bc939a70 \                                  #这个地方的token 和api-server --token-auth-file=/opt/kubernetes/ssl/bootstrap-token.csv 是一样的
\>    --kubeconfig=bootstrap.kubeconfig 
User "kubelet-bootstrap" set.

​	设置上下文参数

[root@master cfssl]# kubectl config set-context default \
\>    --cluster=kubernetes \
\>    --user=kubelet-bootstrap \
\>    --kubeconfig=bootstrap.kubeconfig
	选择默认上下文
[root@master cfssl]# kubectl config use-context default --kubeconfig=bootstrap.kubeconfig
<这个地方也不太明白，只知道用法:是kubelet初次连接api-server进行认证的>

​	copy配置文件
[root@master cfssl]# cp bootstrap.kubeconfig /opt/kubernetes/cfg
[root@master cfssl]# scp bootstrap.kubeconfig root@10.255.0.66:/opt/kubernetes/cfg/
[root@master cfssl]# scp bootstrap.kubeconfig root@10.255.0.92:/opt/kubernetes/cfg/


在node节点上操作,部署kubelet
	1.设置CNI支持
[root@master cfg]# mkdir -p /etc/cni/net.d
[root@node1 net.d]# cat 10-flannel.conflist 
{
  "name": "cbr0",
  "cniVersion":"0.3.1",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}

!!!这个地方有个大坑cniVersion刚开始没有设置版本一致不对，日志没有看错地方，只看了journalctl的日志；最后看到了kubelet自己配置的日志路径下的日志找到问题了；
	2、下载安装cni插件
	也是github上下载cni-plugins-amd64-v0.7.6.tgz
[root@node1 net.d]# tar -xvf cni-plugins-amd64-v0.7.6.tgz -C /opt/cni/bin/	
	3.创建kubelet目录
[root@node1 cfg]# mkdir /data/kubelet
	4.创建kubelet system service 文件
[root@node1 cfg]# cat /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/data/kubelet
ExecStart=/usr/local/bin/kubelet \
  --address=10.255.0.66 \
  --hostname-override=10.255.0.66 \                                        #这个name 是kubectl get nodes 显示的name,可以用ip也可以用主机名node1
  --pod-infra-container-image=mirrorgooglecontainers/pause-amd64:3.0 \
  --experimental-bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \    #这个配置文件
  --kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \                    #这个文件是bootstrap.kubeconfig第一次和api-server认证之后自动生成的
  --cert-dir=/opt/kubernetes/ssl \
  --network-plugin=cni \													#这个就是kubelet定义使用cni插件的地方，有配置文件和bin文件
  --cni-conf-dir=/etc/cni/net.d \
  --cni-bin-dir=/opt/cni/bin \
  --cluster-dns=10.1.0.2 \
  --cluster-domain=cluster.local. \
  --hairpin-mode hairpin-veth \
  --fail-swap-on=false \
  --logtostderr=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target

​	启动服务
[root@node1 cfg]# systemctl daemon-reload
[root@node1 cfg]# systemctl enable kubelet
Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.
[root@node1 cfg]# systemctl start kubelet
[root@node1 cfg]# systemctl status kubelet

​	查看csr请求 注意是在master上执行
​	kubectl get csr
​	批准kubelet 的 TLS 证书请求
​	kubectl get csr|grep 'Pending' | awk 'NR>0{print $1}'| xargs kubectl certificate approve
​	主节点验证
​	这个地方可以设置kube-controller-manager进行自动认证，但是我这边还没实现，以后有机会研究一下；
[root@master ~]# kubectl get nodes
NAME          STATUS   ROLES    AGE   VERSION
10.255.0.66   Ready    <none>   36h   v1.18.8
[root@master ~]# 
\-----------------------------------
​	以上就是从节点安装只需要一个kubelet和cni插件就可以了（好像也可以指定不使用插件）
​	但是要使clusterip能到代理pod就要安装kube-proxy，
​	同理pod之间，pod与node之间的网络要安装flannel来解决；


注意：
	如果kubelet正常的话 kubectl get nodes 就可以看到node是ready的了；和flannel没什么关系；
	到这里出问题了，master没有收到node的请求，等第二个节点看看什么问题；


【kube-proxy】
node1，2操作（如果master节点不配置kube-proxy那么master不通cluster ip,所以master节点最好也启动kube-proxy）
	配置kube-proxy使用LVS
[root@master ~]# yum install  ipvsadm ipset conntrack
master操作
	创建 kube-proxy 证书请求
[root@node1 cfssl]# cat > kube-proxy-csr.json <<EOF
\> {
\>   "CN": "system:kube-proxy",
\>   "hosts": [],
\>   "key": {
\>     "algo": "rsa",
\>     "size": 2048
\>   },
\>   "names": [
\>     {
\>       "C": "CN",
\>       "ST": "BeiJing",
\>       "L": "BeiJing",
\>       "O": "k8s",
\>       "OU": "System"
\>     }
\>   ]
\> }
\> EOF
	生成证书
[root@node1 cfssl]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem    -ca-key=/opt/kubernetes/ssl/ca-key.pem    -config=/opt/kubernetes/ssl/ca-config.json    -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
	分发证书
[root@node1 cfssl]# cp kube-proxy-key.pem kube-proxy.pem /opt/kubernetes/ssl/
[root@node1 cfssl]# scp kube-proxy-key.pem kube-proxy.pem root@10.255.0.92:/opt/kubernetes/ssl/
[root@node1 cfssl]# scp kube-proxy-key.pem kube-proxy.pem root@10.255.0.127:/opt/kubernetes/ssl/

​	创建kube-proxy配置文件
[root@node1 cfssl]# kubectl config set-cluster kubernetes --certificate-authority=/etc/kubernetes/ssl/ca.pem --embed-certs=true --server="[https://10.255.0.127:6443"](https://10.255.0.127:6443) --kubeconfig=kube-proxy.kubeconfig
[root@node1 cfssl]# kubectl config set-credentials kube-proxy    --client-certificate=/opt/kubernetes/ssl/kube-proxy.pem    --client-key=/opt/kubernetes/ssl/kube-proxy-key.pem    --embed-certs=true    --kubeconfig=kube-proxy.kubeconfig
[root@node1 cfssl]# kubectl config set-context default    --cluster=kubernetes    --user=kube-proxy    --kubeconfig=kube-proxy.kubeconfig
[root@node1 cfssl]# kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

​	分发配置文件
[root@node1 cfssl]# cp kube-proxy.kubeconfig /opt/kubernetes/cfg/
[root@node1 cfssl]# scp kube-proxy.kubeconfig root@10.255.0.66:/opt/kubernetes/cfg/
[root@node1 cfssl]# scp kube-proxy.kubeconfig root@10.255.0.66:/opt/kubernetes/cfg/

​	创建kube-proxy system service 文件
​	[root@node2 cfg]# mkdir /data/kube-proxy
[root@node1 log]# cat /usr/lib/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
WorkingDirectory=/data/kube-proxy
ExecStart=/usr/local/bin/kube-proxy \
  --bind-address=10.255.0.66 \
  --hostname-override=10.255.0.66 \
  --kubeconfig=/opt/kubernetes/cfg/kube-proxy.kubeconfig \
  --masquerade-all \
  --feature-gates=SupportIPVSProxyMode=true \
  --proxy-mode=ipvs \
  --ipvs-min-sync-period=5s \
  --ipvs-sync-period=5s \
  --ipvs-scheduler=rr \
  --logtostderr=true \
  --v=2 \
  --logtostderr=false \
  --log-dir=/opt/kubernetes/log

Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

​	启动服务
[root@node1 ~]# systemctl daemon-reload
[root@node1 ~]# systemctl enable kube-proxy
[root@node1 ~]# systemctl start kube-proxy
[root@node1 ~]# systemctl status kube-proxy
​	检查ipvs的状态
[root@node1 ~]# ipvsadm -L -n 

【Flannel】
	编辑证书请求
[root@node1 ~]# vim flanneld-csr.json
cat > flanneld-csr.json <<EOF
{
  "CN": "flanneld",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
EOF
	生成证书

[root@node1 ~]# cfssl gencert -ca=/opt/kubernetes/ssl/ca.pem \
   -ca-key=/opt/kubernetes/ssl/ca-key.pem \
   -config=/opt/kubernetes/ssl/ca-config.json \
   -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld

​	分发证书	
[root@node1 ~]#	scp flanneld-key.pem flanneld.pem root@10.255.0.66:/opt/kubernetes/ssl/
[root@node1 ~]#	scp flanneld-key.pem flanneld.pem root@10.255.0.92:/opt/kubernetes/ssl/
[root@node1 ~]# cp flanneld-key.pem flanneld.pem /opt/kubernetes/ssl/
​	准备flanneld包
​	github上下载flannel-v0.12.0-linux-amd64.tar.gz
[root@node1 ~]#	tar -xvf flannel-v0.12.0-linux-amd64.tar.gz -C /opt/flannel/
[root@node1 ~]#	cp mk-docker-opts.sh /opt/kubernetes/bin/
[root@node1 ~]#	cp flanneld /usr/local/bin/	
​	其他两个节点都这样操作 flanneld 和 mk-docker-opts.sh
​	其实还漏了rm-docker0.sh脚本这个脚本我没找到！！！这就是后来为什么冲突的原因吧

​	配置Flannel配置文件
[root@node1 flanneld]# cat /opt/flanneld/cfg/flannel
FLANNEL_ETCD="-etcd-endpoints=[https://10.255.0.127:2379,https://10.255.0.66:2379,https://10.255.0.92:2379"](https://10.255.0.127:2379,https://10.255.0.66:2379,https://10.255.0.92:2379)
FLANNEL_ETCD_KEY="-etcd-prefix=/kubernetes/network"
FLANNEL_ETCD_CAFILE="--etcd-cafile=/opt/kubernetes/ssl/ca.pem"
FLANNEL_ETCD_CERTFILE="--etcd-certfile=/opt/kubernetes/ssl/flanneld.pem"
FLANNEL_ETCD_KEYFILE="--etcd-keyfile=/opt/kubernetes/ssl/flanneld-key.pem"
复制到其他节点
[root@node1 flanneld]# scp flannel root@10.255.0.66:/opt/flanneld/cfg/
[root@node1 flanneld]# scp flannel root@10.255.0.92:/opt/flanneld/cfg/

​	编辑flannel system文件
[root@node1 flanneld]# cat /usr/lib/systemd/system/flannel.service
[Unit]
Description=Flanneld overlay address etcd agent
After=network.target
Before=docker.service

[Service]
EnvironmentFile=-/opt/flanneld/cfg/flannel
ExecStart=/usr/local/bin/flanneld ${FLANNEL_ETCD} ${FLANNEL_ETCD_KEY} ${FLANNEL_ETCD_CAFILE} ${FLANNEL_ETCD_CERTFILE} ${FLANNEL_ETCD_KEYFILE}
ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -d /run/flannel/docker

Type=notify

[Install]
WantedBy=multi-user.target
RequiredBy=docker.service

​	创建Etcd的key
[root@node1 flanneld]# /opt/kubernetes/bin/etcdctl --ca-file /opt/kubernetes/ssl/ca.pem \
​    --cert-file /opt/kubernetes/ssl/flanneld.pem \
​    --key-file /opt/kubernetes/ssl/flanneld-key.pem \
​    --no-sync -C https://10.255.0.127:2379,https://10.255.0.66:2379,https://10.255.0.92:2379 \
​    mk /kubernetes/network/config '{ "Network": "10.2.0.0/16", "Backend": { "Type": "vxlan", "VNI": 1 }}'	

也是复制到其他节点；
		启动服务

[root@node1 flanneld]# systemctl daemon-reload
[root@node1 flanneld]# systemctl enable flannel
[root@node1 flanneld]# systemctl start flannel
[root@node1 flanneld]# systemctl status flannel.service

配置docker0 和cni0  
这个地方就有点不太明白了，我先再查找下










【后续】
kubernetes命令补全
[root@master kubeInstance]# yum install -y bash-completion
[root@master kubeInstance]# source /usr/share/bash-completion/bash_completion
[root@master kubeInstance]# source <(kubectl completion bash)
[root@master kubeInstance]# echo "source <(kubectl completion bash)" >> ~/.bashrc
[root@master kubeInstance]# vim   ~/.bashrc 
[root@master kubeInstance]# source <( kubectl completion bash | sed 's/kubectl/k/g' )


卸载docker0
ifconfig docker0 down
brctl delbr docker0

brctl命令不存在的话
yum install bridge-utils -y

目前docker0和flannel0冲突

docker0 和cni0 的区别
docker0 的网段是172
cni0的网段是node的网络模式
这种情况是可以的，唯一有问题的是，docker单独启动的容器在其他节点不能访问
最好的情况是docker和k8s共用一个网桥比如：docker0；


创建 kube-apiserver 使用的客户端 token 文件
[root@master cfssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' '
0bc572d207243737a49868f0bc939a70
[root@master cfssl]# cat /opt/kubernetes/ssl/bootstrap-token.csv
0bc572d207243737a49868f0bc939a70,kubelet-bootstrap,10001,"system:kubelet-bootstrap"

创建基础用户名/密码认证配置
[root@master cfssl]# cat /opt/kubernetes/ssl/basic-auth.csv
admin,admin,1
readonly,readonly,2


[root@master cfssl]# kubectl config set-credentials kubelet-bootstrap \
\>    --token=0bc572d207243737a49868f0bc939a70 \
\>    --kubeconfig=bootstrap.kubeconfig 
User "kubelet-bootstrap" set.


[root@master cfssl]# kubectl config set-credentials admin \
\>    --client-certificate=/opt/kubernetes/ssl/admin.pem \
\>    --embed-certs=true \
\>    --client-key=/opt/kubernetes/ssl/admin-key.pem

理解：
admin 用的admin.pem认证的

为了省事，kubelet用的bootstrap，来做认证然后自动生成kubelet-client-2020-09-15-10-22-02.pem文件
api-server 的bootstrap-token.csv 和kubelet 的kubelet-bootstrap 是做一一对应的；

关于remove-docker0.sh

/usr/local/src/kubernetes/cluster/centso/node/bin/